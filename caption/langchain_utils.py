"""
Langchainê³¼ VLM ëª¨ë¸ í†µí•©ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

from typing import Annotated
from pydantic import BaseModel
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI

from .models.master_data import DeepCaptioningTopOutput, SimpleAttributeOutput
import os
import logging

logger = logging.getLogger(__name__)

def setup_langsmith_tracing(
    enable_tracing: bool = True,
    project_name: str|None = None,
    # api_key: Optional[str] = None
) -> None:
    """
    LangSmith tracing ì„¤ì •
    
    Args:
        enable_tracing: tracing í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        project_name: LangSmith í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: None - í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    """
    
    if enable_tracing:
        # LangSmith tracing í™œì„±í™”
        os.environ["LANGSMITH_TRACING"] = "true"
        
        # í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
        if project_name:
            os.environ["LANGSMITH_PROJECT"] = project_name
        elif not os.getenv("LANGSMITH_PROJECT"):
            # ê¸°ë³¸ í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
            os.environ["LANGSMITH_PROJECT"] = "fashion-image-analysis"
        
        current_project = os.getenv("LANGSMITH_PROJECT")
        logger.info(f"âœ… LangSmith tracing í™œì„±í™”ë¨ - í”„ë¡œì íŠ¸: {current_project}")
        
    else:
        # LangSmith tracing ë¹„í™œì„±í™”
        os.environ["LANGSMITH_TRACING"] = "false"
        logger.info("ğŸ”’ LangSmith tracing ë¹„í™œì„±í™”ë¨")



def setup_gemini_model(model_name: str = "gemini-2.0-flash-001", temperature: float = 0.1) -> ChatGoogleGenerativeAI:
    """Gemini ëª¨ë¸ ì„¤ì •"""
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    model = ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature
    )
    logger.info(f"âœ… Gemini ëª¨ë¸ ì„¤ì • ì™„ë£Œ: {model_name}")
    return model


def create_vlm_parser(model_class: BaseModel) -> PydanticOutputParser:
    """
    VLM ì¶œë ¥ì„ ìœ„í•œ Pydantic íŒŒì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    return PydanticOutputParser(pydantic_object=model_class)



def create_simple_attribute_prompt() -> ChatPromptTemplate:
    """
    ë‹¨ìˆœ ì†ì„± ì¶”ì¶œ (ìƒ‰ìƒ)ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    parser = create_vlm_parser(SimpleAttributeOutput)
    
    system_message = SystemMessage(content="""
    ë‹¹ì‹ ì€ ìƒí’ˆ ì´ë¯¸ì§€ì—ì„œ ìƒ‰ìƒ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    ì£¼ì–´ì§„ ì´ë¯¸ì§€ë“¤ì„ ë³´ê³  ê° SKUì˜ ìƒ‰ìƒ ì •ë³´ë§Œ ë¶„ì„í•´ì£¼ì„¸ìš”:
    
    1. ì£¼ ìƒ‰ìƒê³¼ ë¶€ ìƒ‰ìƒ ì‹ë³„
    2. ì •í™•í•œ ìƒ‰ìƒëª…ê³¼ HEX ì½”ë“œ
    3. ìƒ‰ìƒ ì†ì„± (ë°ê¸°, ì±„ë„, í†¤ê° ë“±)
    4. í•´ë‹¹ ìƒ‰ìƒì— ë§ëŠ” ê°„ë‹¨í•œ ì„¤ëª…ë¬¸
    
    ì¤‘ìš”: ìƒ‰ìƒ ì •ë³´ë§Œ ì¶”ì¶œí•˜ê³ , ë””ìì¸ì´ë‚˜ ìŠ¤íƒ€ì¼ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    
    {format_instructions}
    """)
    
    human_message = HumanMessage(content="""
    ìƒí’ˆ ê·¸ë£¹ ID: {product_group_id}
    ì´ë¯¸ì§€ ë°ì´í„°: {image_data}
    SKU ì •ë³´: {sku_info}
    """)
    
    return ChatPromptTemplate.from_messages([
        system_message,
        human_message
    ]).partial(format_instructions=parser.get_format_instructions())


# def validate_and_fix_vlm_output(
#     raw_output: str, 
#     target_model: Type[BaseModel]
# ) -> BaseModel:
#     """
#     VLM ì›ì‹œ ì¶œë ¥ì„ ê²€ì¦í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•˜ì—¬ Pydantic ëª¨ë¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
#     Args:
#         raw_output: VLMì˜ ì›ì‹œ ì¶œë ¥ í…ìŠ¤íŠ¸
#         target_model: ëª©í‘œ Pydantic ëª¨ë¸ í´ë˜ìŠ¤
        
#     Returns:
#         BaseModel: ê²€ì¦ëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        
#     Raises:
#         ValueError: íŒŒì‹±ì— ì‹¤íŒ¨í•œ ê²½ìš°
#     """
#     parser = create_vlm_parser(target_model)
    
#     try:
#         # ì§ì ‘ íŒŒì‹± ì‹œë„
#         return parser.parse(raw_output)
#     except Exception as e:
#         # íŒŒì‹± ì‹¤íŒ¨ì‹œ í›„ì²˜ë¦¬ ë¡œì§ ì ìš©
#         # ì—¬ê¸°ì— ì¼ë°˜ì ì¸ VLM ì¶œë ¥ ì˜¤ë¥˜ ìˆ˜ì • ë¡œì§ì„ ì¶”ê°€
#         cleaned_output = _clean_vlm_output(raw_output)
#         try:
#             return parser.parse(cleaned_output)
#         except Exception as e2:
#             raise ValueError(f"VLM ì¶œë ¥ íŒŒì‹± ì‹¤íŒ¨: {e2}") from e2


# def _clean_vlm_output(raw_output: str) -> str:
#     """
#     VLM ì¶œë ¥ì—ì„œ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
#     """
#     # JSON ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±°
#     cleaned = raw_output.strip()
#     if cleaned.startswith("```json"):
#         cleaned = cleaned[7:]
#     if cleaned.endswith("```"):
#         cleaned = cleaned[:-3]
    
#     # ë¶ˆí•„ìš”í•œ ì•ë’¤ ê³µë°± ì œê±°
#     cleaned = cleaned.strip()
    
#     # ê¸°íƒ€ ì¼ë°˜ì ì¸ ì •ë¦¬ ì‘ì—…ë“¤...
    
#     return cleaned


# def get_model_schema_description(model_class: Type[BaseModel]) -> str:
#     """
#     Pydantic ëª¨ë¸ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     schema = model_class.model_json_schema()
    
#     def format_property(name: str, prop: Dict[str, Any], level: int = 0) -> str:
#         indent = "  " * level
#         prop_type = prop.get("type", "unknown")
#         description = prop.get("description", "")
        
#         result = f"{indent}- {name} ({prop_type})"
#         if description:
#             result += f": {description}"
#         result += "\n"
        
#         # ì¤‘ì²©ëœ ê°ì²´ ì²˜ë¦¬
#         if "properties" in prop:
#             for sub_name, sub_prop in prop["properties"].items():
#                 result += format_property(sub_name, sub_prop, level + 1)
                
#         return result
    
#     description = f"ëª¨ë¸: {model_class.__name__}\n\n"
#     description += "í•„ë“œ êµ¬ì¡°:\n"
    
#     for prop_name, prop_info in schema.get("properties", {}).items():
#         description += format_property(prop_name, prop_info)
    
#     return description


# def normalize_vlm_enum_values(data: dict) -> dict:
#     """
#     VLM ì¶œë ¥ì—ì„œ enum ê°’ë“¤ì„ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜
#     """
#     # ìŠ¤íƒ€ì¼ íƒœê·¸ ë§¤í•‘
#     style_tag_mapping = {
#         "ë ˆíŠ¸ë¡œ": "ë¹ˆí‹°ì§€/ë ˆíŠ¸ë¡œ",
#         "ë¹ˆí‹°ì§€": "ë¹ˆí‹°ì§€/ë ˆíŠ¸ë¡œ",
#         "ëª¨ë˜": "ëª¨ë˜/ë¯¸ë‹ˆë©€",
#         "ë¯¸ë‹ˆë©€": "ëª¨ë˜/ë¯¸ë‹ˆë©€",
#         "ë² ì´ì§": "ì‹¬í”Œ ë² ì´ì§",
#         "ì‹¬í”Œ": "ì‹¬í”Œ ë² ì´ì§",
#         "í¬ë©€": "í¬ë©€/í´ë˜ì‹",
#         "í´ë˜ì‹": "í¬ë©€/í´ë˜ì‹",
#         "ìŠ¤í¬í‹°": "ìŠ¤í¬í‹°/ì• ìŠ¬ë ˆì €",
#         "ì• ìŠ¬ë ˆì €": "ìŠ¤í¬í‹°/ì• ìŠ¬ë ˆì €",
#         "ë ˆê·¤ëŸ¬": "ë ˆê·¤ëŸ¬ í•/ìŠ¤íƒ ë‹¤ë“œ í•",
#         "ìŠ¤íƒ ë‹¤ë“œ": "ë ˆê·¤ëŸ¬ í•/ìŠ¤íƒ ë‹¤ë“œ í•",
#         "ë ˆê·¤ëŸ¬ í•": "ë ˆê·¤ëŸ¬ í•/ìŠ¤íƒ ë‹¤ë“œ í•",
#         "ìŠ¤íƒ ë‹¤ë“œ í•": "ë ˆê·¤ëŸ¬ í•/ìŠ¤íƒ ë‹¤ë“œ í•"
#     }
    
#     # TPO íƒœê·¸ ë§¤í•‘
#     tpo_tag_mapping = {
#         "ì˜¤í”¼ìŠ¤": "ì˜¤í”¼ìŠ¤/ë¹„ì¦ˆë‹ˆìŠ¤",
#         "ë¹„ì¦ˆë‹ˆìŠ¤": "ì˜¤í”¼ìŠ¤/ë¹„ì¦ˆë‹ˆìŠ¤",
#         "ê²©ì‹": "ê²©ì‹/í•˜ê°",
#         "í•˜ê°": "ê²©ì‹/í•˜ê°",
#         "ë°ì´íŠ¸": "ë°ì´íŠ¸/ì£¼ë§",
#         "ì£¼ë§": "ë°ì´íŠ¸/ì£¼ë§",
#         "ì—¬í–‰": "ì—¬í–‰/íœ´ê°€",
#         "íœ´ê°€": "ì—¬í–‰/íœ´ê°€",
#         "íŒŒí‹°": "íŒŒí‹°/ëª¨ì„",
#         "ëª¨ì„": "íŒŒí‹°/ëª¨ì„",
#         "í™ˆì›¨ì–´": "í™ˆì›¨ì–´/ë¼ìš´ì§€",
#         "ë¼ìš´ì§€": "í™ˆì›¨ì–´/ë¼ìš´ì§€"
#     }
    
#     def normalize_tags(tags: list, mapping: dict) -> list:
#         """íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”"""
#         if not isinstance(tags, list):
#             return tags
        
#         normalized = []
#         for tag in tags:
#             if isinstance(tag, str):
#                 # ì •í™•í•œ ë§¤í•‘ì´ ìˆìœ¼ë©´ ì‚¬ìš©
#                 if tag in mapping:
#                     normalized.append(mapping[tag])
#                 else:
#                     normalized.append(tag)
#             else:
#                 normalized.append(tag)
#         return normalized
    
#     # ë°ì´í„° ì •ê·œí™”
#     if isinstance(data, dict):
#         # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
#         for key, value in data.items():
#             if key == "style_tags" and isinstance(value, list):
#                 data[key] = normalize_tags(value, style_tag_mapping)
#             elif key == "tpo_tags" and isinstance(value, list):
#                 data[key] = normalize_tags(value, tpo_tag_mapping)
#             elif key == "fit" and isinstance(value, str):
#                 # í• íƒ€ì… ì •ê·œí™”
#                 if value in style_tag_mapping:
#                     data[key] = style_tag_mapping[value]
#             elif isinstance(value, dict):
#                 data[key] = normalize_vlm_enum_values(value)
#             elif isinstance(value, list):
#                 data[key] = [normalize_vlm_enum_values(item) if isinstance(item, dict) else item for item in value]
    
#     return data 