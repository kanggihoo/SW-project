'''
ë¡œì»¬ì—ì„œ ê°€ì§€ê³  ìˆì–´ì•¼ í•˜ëŠ”ê±° 
1. í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° (simple , detail) json íŒŒì¼ 
2. dynmodbë¡œ ë¶€í„° ì„ íƒëœ ëŒ€í‘œ ì´ë¯¸ì§€ ì •ë³´ 
3. ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ë¶€í„° ìƒì„±í•œ ìº¡ì…˜ ì •ë³´ 
4. ìº¡ì…˜ ì •ë³´ë¡œ ë¶€í„° ìƒì„±í•œ ì„ë² ë”© ì •ë³´ 

mongodb atlasì— ì €ì¥í•´ì•¼ í•˜ëŠ” ê²ƒ?

'''

'''
simple_product_summary.csv ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ 

detail_product_summary.csv ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ 


'''
import logging
import pandas as pd
import json
from typing import Dict, Any, Optional
import logging
from pathlib import Path
logging.basicConfig(level=logging.INFO , format='%(asctime)s - %(name)s - [%(levelname)s] - %(message)s : %(filename)s - %(lineno)d' , datefmt='%Y-%m-%d %H:%M:%S')
import sys
sys.path.append(str(Path(__file__).parent.parent))
logger = logging.getLogger(__name__)
from aws.dynamodb import DynamoDBManager
from db import create_fashion_repo
from aws.config import Config
from tqdm import tqdm
from pymongo import UpdateOne

def load_json_to_df(file_path: str , index_column_name: str = "product_id") -> pd.DataFrame:
    """
    JSON íŒŒì¼ì„ pandas DataFrameìœ¼ë¡œ ì½ì–´ì„œ product_idë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    
    Args:
        file_path (str): JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        pd.DataFrame: product_idë¥¼ ì¸ë±ìŠ¤ë¡œ ê°€ì§„ DataFrame
    """
    try:
        df = pd.read_json(file_path , encoding="utf-8")
        
        # # product_idê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        # if 'product_id' not in df.columns:
        #     raise ValueError(f"product_id ì»¬ëŸ¼ì´ {file_path}ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # # product_idë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
        # df.set_index(index_column_name, inplace=True)
        
        logger.info(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ - {len(df)} ê°œì˜ ì œí’ˆ ë°ì´í„°")
        logger.info(f"ğŸ“Š ì»¬ëŸ¼: {list(df.columns)}")
        
        return df
        
    except FileNotFoundError:
        logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return pd.DataFrame()
    except json.JSONDecodeError:
        logger.error(f"âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def _default_csv_projection_fields(columns: list[str])->list[str]:
    columns = set(columns)
    projection_columns = [
        "product_id",
        "product_price",
        "product_original_price",
        "product_discount_price",
        "product_brand_name",
        "product_name",
        "num_likes",
        "avg_rating",
        "review_count",
        "category_main",
        "category_sub",
        "gender",
        "detail_text",
        "review_texts",
        "size_detail_info",
        "fit_info",
        "color_size_info",
        "success_status"
    ]
    
    if all(col in columns for col in projection_columns):
        logger.info(f"summary_csv_projection_fields ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëª¨ë‘ ìˆìŠµë‹ˆë‹¤. : {projection_columns}")
        return projection_columns
    else:
        raise ValueError(f"summary_csv_projection_fields ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. : {columns}")
# def _default_dynamodb_projection_fields(columns: list[str])->list[str]:
#     columns = set(columns)
#     projection_columns = [
#         "caption_status",
#         "curation_status",
#         "product_original_price",
#         "product_discount_price",
#     ]

def _rename_columns(df: pd.DataFrame)->pd.DataFrame:

    mapping_columns = {
        "category_main": "main_category",
        "category_sub": "sub_category",
        "success_status" : "crawling_status",
        # mongoDB í˜¸í™˜ì„± 
        "product_id" : "_id",
    }
    if all(col in df.columns for col in mapping_columns.keys()):
        try:
            df = df.rename(columns=mapping_columns)
            logger.info(f"ì»¬ëŸ¼ëª… ë³€ê²½ì™„ë£Œ : {mapping_columns}")
            return df
        except Exception as e:
            raise ValueError(f"rename_columns ë„ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        missing_cols = [col for col in mapping_columns.keys() if col not in df.columns]
        raise ValueError(f"rename_columnsì— í•„ìš”í•œ ì»¬ëŸ¼ì´ DataFrameì— ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
        

def _add_new_columns(df: pd.DataFrame , new_columns: str=None)->pd.DataFrame:
    if new_columns is None:
        new_columns = "data_status"

    df[new_columns] = df.apply(lambda x : "CR_DET" if x["crawling_status"]=="success" else "CR_SUM" , axis = 1)
    return df
def _add_new_none_columns(df: pd.DataFrame , new_columns: str=None)->pd.DataFrame:
    if new_columns is None:
        new_columns = "representative_assets"

    df[new_columns] = None
    return df
def _process_df(df: pd.DataFrame)->pd.DataFrame:
    projection_columns = _default_csv_projection_fields(df.columns)
    df = df[projection_columns]
    df = _rename_columns(df)
    df = _add_new_columns(df , new_columns="data_status")
    df = _add_new_none_columns(df , new_columns="representative_assets")
    return df

if __name__ == "__main__":

    # ì´ˆê¸° ì„¤ì •
    ddb_config = Config().get_dynamodb_config()
    dynamodb_client = DynamoDBManager(ddb_config["region_name"],ddb_config["table_name"] , ddb_config)
    mongodb = create_fashion_repo(use_atlas=False)
    # =============================================================================
    # ë°ì´í„° ê²½ë¡œ ì„¤ì • 
    # =============================================================================
    BASE_DIR = Path("/Users/kkh/Desktop/musinsa-crawling/data")
    main_category = "ìƒì˜"
    sub_category = "ì…”ì¸ -ë¸”ë¼ìš°ìŠ¤"
    detail_json_file_name = f"musinsa_product_detail_{main_category}_{sub_category}.json"
    detail_json_path = BASE_DIR / detail_json_file_name

    # =============================================================================
    # ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸° ì „ì²˜ë¦¬ 
    # ============================================================================
    product_detail_df = load_json_to_df(detail_json_path)
    product_detail_df = _process_df(product_detail_df)
    total = len(product_detail_df)
    # =============================================================================
    # ë°˜ë³µë¬¸ì„ í†µí•´ dynamodbë¡œ ë¶€í„° ë°ì´í„° ì¡°íšŒ 
    # =============================================================================
    BATCH_SIZE = 1000
    # with tqdm(total=len(product_detail_df), desc="ë°ì´í„° ì¡°íšŒ ì¤‘") as pbar:
    for batch_index in range(0, total, BATCH_SIZE):
        batch_df = product_detail_df.iloc[batch_index:batch_index+BATCH_SIZE]
        updates_map = {}
        for row in batch_df.itertuples(index=True):
            index = row.Index
            product_id = str(row[1])
            sub_category = int(row.sub_category)
            result_dynamo = dynamodb_client.get_item(sub_category=sub_category, product_id=product_id)
            if result_dynamo is None:
                continue
            curation_status = result_dynamo.get("curation_status")
            update_data = {}
            if curation_status == "COMPLETED":
                # ëˆ„êµ°ê°€ê°€ curation ì‘ì—…ì„ ì™„ë£Œ í•œ ê²½ìš° : ëŒ€í‘œ ì´ë¯¸ì§€ ì¡°íšŒ 
                representative_assets = result_dynamo.get("representative_assets")
                update_data["representative_assets"] = representative_assets
                update_data["data_status"] = "RE_COMP"
                updates_map[index] = update_data
            elif curation_status == "PASS" or curation_status == "PENDING":
                # dynamodbì— ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° 
                update_data["data_status"] = "AWS_UPL"
                updates_map[index] = update_data
            else:
                raise ValueError(f"curation_status ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. : {curation_status}")

        if updates_map:
            update_df = pd.DataFrame.from_dict(updates_map, orient="index")
            batch_df.update(update_df)
            logger.info(f"ë°°ì¹˜ ì™„ë£Œ : {batch_index} - {batch_index+BATCH_SIZE}")

        # mongodb ì— ì €ì¥ 
        mongo_operations = [] 
        batch_docs = batch_df.to_dict(orient="records")

        for doc in batch_docs:
            filter_query = {"_id": str(doc["_id"])}
            update_data = {k:v for k ,v in doc.items() if k != "_id"}
            update_operation = {"$set" : update_data}
            mongo_operations.append(
                UpdateOne(filter_query, update_operation , upsert=True)
            )

        if mongo_operations:
            try:
                # ordered=Falseë¡œ ì„¤ì •í•˜ì—¬ ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                result = mongodb.collection.bulk_write(mongo_operations, ordered=False)
                
                # ì„±ê³µí•œ ì‘ì—…ë“¤ ë¡œê¹…
                logger.info(f"âœ… ë°°ì¹˜ {batch_index}-{batch_index+BATCH_SIZE} bulk_write ì„±ê³µ:")
                logger.info(f"   - ìˆ˜ì •ëœ ë¬¸ì„œ: {result.modified_count}")
                logger.info(f"   - ì‚½ì…ëœ ë¬¸ì„œ: {result.upserted_count}")
                logger.info(f"   - ë§¤ì¹­ëœ ë¬¸ì„œ: {result.matched_count}")
                
                # ì‹¤íŒ¨í•œ ì‘ì—…ë“¤ ì²˜ë¦¬
                if hasattr(result, 'bulk_api_result') and 'writeErrors' in result.bulk_api_result:
                    failed_docs = []
                    for error in result.bulk_api_result['writeErrors']:
                        error_index = error['index']
                        error_doc = batch_docs[error_index]
                        failed_docs.append({
                            '_id': str(error_doc['_id']),
                            'error_code': error['code'],
                            'error_message': error['errmsg']
                        })
                        logger.error(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨ - _id: {error_doc['_id']}, ì˜¤ë¥˜: {error['errmsg']}")
                    
                    # ì‹¤íŒ¨í•œ ë¬¸ì„œë“¤ì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
                    if failed_docs:
                        failed_file_path = f"failed_docs_batch_{batch_index}_{batch_index+BATCH_SIZE}.json"
                        with open(failed_file_path, 'w', encoding='utf-8') as f:
                            json.dump(failed_docs, f, ensure_ascii=False, indent=2)
                        logger.warning(f"âš ï¸ ì‹¤íŒ¨í•œ ë¬¸ì„œ {len(failed_docs)}ê°œë¥¼ {failed_file_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {batch_index}-{batch_index+BATCH_SIZE} bulk_write ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì‹¤íŒ¨í•œ ë¬¸ì„œë“¤ì„ ì €ì¥
                failed_docs = []
                for i, doc in enumerate(batch_docs):
                    failed_docs.append({
                        '_id': str(doc['_id']),
                        'error_code': 'EXCEPTION',
                        'error_message': str(e)
                    })
                
                failed_file_path = f"failed_docs_batch_{batch_index}_{batch_index+BATCH_SIZE}_exception.json"
                with open(failed_file_path, 'w', encoding='utf-8') as f:
                    json.dump(failed_docs, f, ensure_ascii=False, indent=2)
                logger.error(f"âŒ ë°°ì¹˜ ì „ì²´ ì‹¤íŒ¨ - {len(failed_docs)}ê°œ ë¬¸ì„œë¥¼ {failed_file_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"ë°°ì¹˜ ì™„ë£Œ : {batch_index} - {batch_index+BATCH_SIZE}")


  