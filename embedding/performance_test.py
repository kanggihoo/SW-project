import asyncio
import time
import statistics
import aiohttp
from typing import List, Dict, Any
import json
from dataclasses import dataclass
from datetime import datetime
import os
from dotenv import load_dotenv

# Import embedding functions
from embedding.embedding import JinaEmbedding
from embedding.other_api import get_embedding_with_openai, get_embedding_with_gemini

load_dotenv()

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    api_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    median_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    total_tokens: int
    avg_tokens_per_request: float
    errors: List[str]
    timestamp: str

class EmbeddingPerformanceTester:
    """ì„ë² ë”© API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.test_texts = [
            "Hello, world!",
            "This is a test sentence for embedding performance measurement.",
            "The quick brown fox jumps over the lazy dog.",
            "Artificial intelligence and machine learning are transforming the world.",
            "Natural language processing enables computers to understand human language.",
            "Vector embeddings represent text as numerical vectors in high-dimensional space.",
            "Semantic similarity can be measured using cosine similarity between embeddings.",
            "Deep learning models have revolutionized the field of natural language processing.",
            "Transformer architecture has become the foundation for modern NLP models.",
            "Attention mechanisms allow models to focus on relevant parts of input sequences."
        ]
        
        # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
        self.long_texts = [
            "This is a much longer text that contains more words and should take longer to process. " * 10,
            "Another long text with different content and structure. " * 15,
            "A comprehensive text about machine learning and artificial intelligence. " * 20
        ]
        
        self.results: List[PerformanceMetrics] = []
    
    async def test_jina_embedding(self, session: aiohttp.ClientSession, 
                                 texts: List[str], test_name: str = "Jina Embedding") -> PerformanceMetrics:
        """Jina Embedding API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª Testing {test_name}...")
        
        jina_embedding = JinaEmbedding()
        response_times = []
        errors = []
        successful_requests = 0
        total_tokens = 0
        
        start_time = time.time()
        
        for i, text in enumerate(texts):
            try:
                request_start = time.time()
                
                # ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬
                result = await jina_embedding.get_embedding([text], session)
                
                request_time = time.time() - request_start
                response_times.append(request_time)
                successful_requests += 1
                
                # í† í° ìˆ˜ ê³„ì‚° (ëŒ€ëµì ì¸ ì¶”ì •)
                total_tokens += len(text.split())
                
                print(f"  âœ“ Request {i+1}/{len(texts)}: {request_time:.3f}s")
                
            except Exception as e:
                errors.append(f"Request {i+1}: {str(e)}")
                print(f"  âœ— Request {i+1}/{len(texts)}: Failed - {str(e)}")
        
        total_time = time.time() - start_time
        
        return self._calculate_metrics(
            test_name, len(texts), successful_requests, len(errors),
            total_time, response_times, total_tokens, errors
        )
    
    def test_openai_embedding(self, texts: List[str], test_name: str = "OpenAI Embedding") -> PerformanceMetrics:
        """OpenAI Embedding API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª Testing {test_name}...")
        
        response_times = []
        errors = []
        successful_requests = 0
        total_tokens = 0
        
        start_time = time.time()
        
        for i, text in enumerate(texts):
            try:
                request_start = time.time()
                
                result = get_embedding_with_openai([text])
                
                request_time = time.time() - request_start
                response_times.append(request_time)
                successful_requests += 1
                
                # í† í° ìˆ˜ ê³„ì‚° (ëŒ€ëµì ì¸ ì¶”ì •)
                total_tokens += len(text.split())
                
                print(f"  âœ“ Request {i+1}/{len(texts)}: {request_time:.3f}s")
                
            except Exception as e:
                errors.append(f"Request {i+1}: {str(e)}")
                print(f"  âœ— Request {i+1}/{len(texts)}: Failed - {str(e)}")
        
        total_time = time.time() - start_time
        
        return self._calculate_metrics(
            test_name, len(texts), successful_requests, len(errors),
            total_time, response_times, total_tokens, errors
        )
    
    def test_gemini_embedding(self, texts: List[str], test_name: str = "Gemini Embedding") -> PerformanceMetrics:
        """Gemini Embedding API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª Testing {test_name}...")
        
        response_times = []
        errors = []
        successful_requests = 0
        total_tokens = 0
        
        start_time = time.time()
        
        for i, text in enumerate(texts):
            try:
                request_start = time.time()
                
                result = get_embedding_with_gemini([text])
                
                request_time = time.time() - request_start
                response_times.append(request_time)
                successful_requests += 1
                
                # í† í° ìˆ˜ ê³„ì‚° (ëŒ€ëµì ì¸ ì¶”ì •)
                total_tokens += len(text.split())
                
                print(f"  âœ“ Request {i+1}/{len(texts)}: {request_time:.3f}s")
                
            except Exception as e:
                errors.append(f"Request {i+1}: {str(e)}")
                print(f"  âœ— Request {i+1}/{len(texts)}: Failed - {str(e)}")
        
        total_time = time.time() - start_time
        
        return self._calculate_metrics(
            test_name, len(texts), successful_requests, len(errors),
            total_time, response_times, total_tokens, errors
        )
    
    def _calculate_metrics(self, api_name: str, total_requests: int, successful_requests: int,
                          failed_requests: int, total_time: float, response_times: List[float],
                          total_tokens: int, errors: List[str]) -> PerformanceMetrics:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not response_times:
            return PerformanceMetrics(
                api_name=api_name,
                total_requests=total_requests,
                successful_requests=successful_requests,
                failed_requests=failed_requests,
                total_time=total_time,
                avg_response_time=0,
                min_response_time=0,
                max_response_time=0,
                median_response_time=0,
                p95_response_time=0,
                p99_response_time=0,
                requests_per_second=0,
                total_tokens=total_tokens,
                avg_tokens_per_request=total_tokens / total_requests if total_requests > 0 else 0,
                errors=errors,
                timestamp=datetime.now().isoformat()
            )
        
        sorted_times = sorted(response_times)
        n = len(sorted_times)
        
        return PerformanceMetrics(
            api_name=api_name,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            total_time=total_time,
            avg_response_time=statistics.mean(response_times),
            min_response_time=min(response_times),
            max_response_time=max(response_times),
            median_response_time=statistics.median(response_times),
            p95_response_time=sorted_times[int(0.95 * n)] if n > 0 else 0,
            p99_response_time=sorted_times[int(0.99 * n)] if n > 0 else 0,
            requests_per_second=successful_requests / total_time if total_time > 0 else 0,
            total_tokens=total_tokens,
            avg_tokens_per_request=total_tokens / successful_requests if successful_requests > 0 else 0,
            errors=errors,
            timestamp=datetime.now().isoformat()
        )
    
    def print_metrics(self, metrics: PerformanceMetrics):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶œë ¥"""
        print(f"\nğŸ“Š {metrics.api_name} Performance Results:")
        print("=" * 60)
        print(f"ğŸ“ˆ Requests: {metrics.successful_requests}/{metrics.total_requests} successful")
        print(f"â±ï¸  Total Time: {metrics.total_time:.3f}s")
        print(f"ğŸš€ Requests/sec: {metrics.requests_per_second:.2f}")
        print(f"ğŸ“Š Response Times:")
        print(f"   â€¢ Average: {metrics.avg_response_time:.3f}s")
        print(f"   â€¢ Median: {metrics.median_response_time:.3f}s")
        print(f"   â€¢ Min: {metrics.min_response_time:.3f}s")
        print(f"   â€¢ Max: {metrics.max_response_time:.3f}s")
        print(f"   â€¢ 95th percentile: {metrics.p95_response_time:.3f}s")
        print(f"   â€¢ 99th percentile: {metrics.p99_response_time:.3f}s")
        print(f"ğŸ“ Tokens: {metrics.total_tokens} total, {metrics.avg_tokens_per_request:.1f} avg/request")
        
        if metrics.errors:
            print(f"âŒ Errors ({len(metrics.errors)}):")
            for error in metrics.errors[:3]:  # ìµœëŒ€ 3ê°œ ì—ëŸ¬ë§Œ í‘œì‹œ
                print(f"   â€¢ {error}")
            if len(metrics.errors) > 3:
                print(f"   â€¢ ... and {len(metrics.errors) - 3} more errors")
    
    def save_results(self, filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"embedding_performance_results_{timestamp}.json"
        
        results_dict = []
        for metrics in self.results:
            results_dict.append({
                "api_name": metrics.api_name,
                "total_requests": metrics.total_requests,
                "successful_requests": metrics.successful_requests,
                "failed_requests": metrics.failed_requests,
                "total_time": metrics.total_time,
                "avg_response_time": metrics.avg_response_time,
                "min_response_time": metrics.min_response_time,
                "max_response_time": metrics.max_response_time,
                "median_response_time": metrics.median_response_time,
                "p95_response_time": metrics.p95_response_time,
                "p99_response_time": metrics.p99_response_time,
                "requests_per_second": metrics.requests_per_second,
                "total_tokens": metrics.total_tokens,
                "avg_tokens_per_request": metrics.avg_tokens_per_request,
                "errors": metrics.errors,
                "timestamp": metrics.timestamp
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {filename}")
    
    async def run_comprehensive_test(self):
        """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ Starting Comprehensive Embedding Performance Test")
        print("=" * 60)
        
        # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ“ Phase 1: Basic Text Test")
        print("-" * 40)
        
        async with aiohttp.ClientSession() as session:
            # Jina Embedding í…ŒìŠ¤íŠ¸
            jina_metrics = await self.test_jina_embedding(session, self.test_texts)
            self.results.append(jina_metrics)
            self.print_metrics(jina_metrics)
        
        # OpenAI Embedding í…ŒìŠ¤íŠ¸
        try:
            openai_metrics = self.test_openai_embedding(self.test_texts)
            self.results.append(openai_metrics)
            self.print_metrics(openai_metrics)
        except Exception as e:
            print(f"âŒ OpenAI test failed: {e}")
        
        # Gemini Embedding í…ŒìŠ¤íŠ¸
        try:
            gemini_metrics = self.test_gemini_embedding(self.test_texts)
            self.results.append(gemini_metrics)
            self.print_metrics(gemini_metrics)
        except Exception as e:
            print(f"âŒ Gemini test failed: {e}")
        
        # 2. ê¸´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ“ Phase 2: Long Text Test")
        print("-" * 40)
        
        async with aiohttp.ClientSession() as session:
            jina_long_metrics = await self.test_jina_embedding(
                session, self.long_texts, "Jina Embedding (Long Text)"
            )
            self.results.append(jina_long_metrics)
            self.print_metrics(jina_long_metrics)
        
        # 3. ê²°ê³¼ ì €ì¥
        self.save_results()
        
        # 4. ìš”ì•½ ì¶œë ¥
        self.print_summary()
    
    def print_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ¯ Performance Test Summary")
        print("=" * 60)
        
        # ì„±ê³µë¥  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(self.results, key=lambda x: x.requests_per_second, reverse=True)
        
        print(f"{'API':<25} {'RPS':<10} {'Avg Time':<12} {'Success Rate':<15}")
        print("-" * 60)
        
        for metrics in sorted_results:
            success_rate = (metrics.successful_requests / metrics.total_requests) * 100
            print(f"{metrics.api_name:<25} {metrics.requests_per_second:<10.2f} "
                  f"{metrics.avg_response_time:<12.3f} {success_rate:<15.1f}%")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = EmbeddingPerformanceTester()
    await tester.run_comprehensive_test()

if __name__ == "__main__":
    asyncio.run(main()) 